{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import cli_args\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb.init (\n",
    "# # set the wand project where this run will be logged\n",
    "# project=\"CS6910_Assignment_1\",\n",
    "# )\n",
    "# # Loading the fashion-MNIST dataset\n",
    "# (x_train, y_train), (_test, _test) = fashion_mnist.load_data ()\n",
    "# #class names for fashion-MIST\n",
    "# class_names = ['T-shirt',\n",
    "# 'Trouser',\n",
    "# 'Pullover',\n",
    "# 'Dress',\n",
    "# 'Sandal', 'Shirt',\n",
    "# 'Sneaker',\n",
    "# 'Bag',\n",
    "# 'Coat',\n",
    "# 'Ankle boot']\n",
    "# # creating 2x5 grid\n",
    "# img={}\n",
    "# for i in range(10):\n",
    "#     # to find first image in the training set with class label i\n",
    "#     idx = np.where (y_train == i)[0][0]\n",
    "#     # Plot the image\n",
    "#     img[class_names[i]] = wandb.Image(x_train[idx], caption=class_names[i])\n",
    "# wandb.log(img)\n",
    "# # [optional] finish the wand run, necessary in notebooks\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-wp WANDB_PROJECT] [-we WANDB_ENTITY]\n",
      "                             [-d DATASET] [-e EPOCHS] [-b BATCH_SIZE]\n",
      "                             [-l LOSS] [-o OPTIMIZER] [-lr LEARNING_RATE]\n",
      "                             [-m MOMENTUM] [-beta BETA] [-beta1 BETA1]\n",
      "                             [-beta2 BETA2] [-eps EPSILON] [-w_d WEIGHT_DECAY]\n",
      "                             [-w_i WEIGHT_INIT] [-nhl NUM_LAYERS]\n",
      "                             [-sz HIDDEN_SIZE [HIDDEN_SIZE ...]]\n",
      "                             [-a ACTIVATION] [-oa OUTPUT_ACTIVATION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/vanditshah/Library/Jupyter/runtime/kernel-794a2b15-074b-4a45-bd73-f47f686c1e29.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "arguments = cli_args.argumentsIntake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def one_hot_encoder(self, num_of_classes):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActiviationFunction:\n",
    "    def __init__(self, name):\n",
    "        self.act_name = name\n",
    "    \n",
    "    @property\n",
    "    def setname(self, new_name):\n",
    "        self.act_name = new_name\n",
    "\n",
    "    def activate(self, data, backprop = False):\n",
    "        if self.act_name == \"identity\" and backprop:\n",
    "            pass\n",
    "        elif self.act_name == \"identity\" and not backprop:\n",
    "            return self.identity(data)\n",
    "        elif self.act_name == \"sigmoid\" and backprop:\n",
    "            return self.backprop_sigmoid(data)\n",
    "        elif self.act_name == \"sigmoid\" and not backprop:\n",
    "            return self.sigmoid(data)\n",
    "        elif self.act_name == \"tanh\" and backprop:\n",
    "            return self.backprop_tanh(data)\n",
    "        elif self.act_name == \"tanh\" and not backprop:\n",
    "            return self.tanh(data)\n",
    "        elif self.act_name == \"ReLU\" and backprop:\n",
    "            pass\n",
    "        elif self.act_name == \"ReLU\" and not backprop:\n",
    "            return self.tanh(data)\n",
    "\n",
    "    \n",
    "    def identity(self, data):\n",
    "        return data\n",
    "    \n",
    "    def backprop_identity(self, data):\n",
    "        pass\n",
    "\n",
    "    def sigmoid(self, data):\n",
    "        return 1/(1 + np.exp(-data))\n",
    "    \n",
    "    def backprop_sigmoid(self, data):\n",
    "        temp = self.sigmoid(data)\n",
    "        return (1 - temp)*temp\n",
    "    \n",
    "    def relu(self, data):\n",
    "        return np.max(0, data)\n",
    "    \n",
    "    def backprop_relu(self, data):\n",
    "        return np.heaviside(data, 1)\n",
    "    \n",
    "    def tanh(self, data):\n",
    "        return np.tanh(data)\n",
    "    \n",
    "    def backprop_tanh(self, data):\n",
    "        return 1 - np.square(np.tanh(data))\n",
    "    \n",
    "    def softmax(self, data):\n",
    "        _max = np.max(data)\n",
    "        numerator = np.exp(data - _max)\n",
    "        denominator = np.sum(numerator)\n",
    "        return numerator/denominator\n",
    "\n",
    "    def backprop_softmax(self, data):\n",
    "        _softmax = self.softmax(data)\n",
    "        return _softmax * (1 - _softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def getLoss(self, y, yhat):\n",
    "        if self.name == \"cross_entropy\":\n",
    "            return self.cross_entropy_loss(y, yhat)\n",
    "        elif self.name == \"mean_squared_error\":\n",
    "            return self.mean_squared_error_loss(y, yhat)\n",
    "    \n",
    "    def getGradient(self, y, yhat):\n",
    "        if self.name == \"cross_entropy\":\n",
    "            return self.backprop_cross_entropy(y, yhat)\n",
    "        elif self.name == \"mean_squared_error\":\n",
    "            return self.backprop_mean_squared_loss(y, yhat)\n",
    "    \n",
    "    def cross_entropy_loss(self, y, yhat):\n",
    "        probability_predicted = yhat[np.argmax(y)]\n",
    "        if probability_predicted <= 0:\n",
    "            probability_predicted += 10**(-9)\n",
    "        return -np.log(probability_predicted)\n",
    "    \n",
    "    def mean_squared_error_loss(self,y, yhat):\n",
    "        return (1/2)*np.sum(np.square(yhat - y))\n",
    "    \n",
    "    def backprop_cross_entropy(self, y, yhat):\n",
    "        return yhat - y\n",
    "    \n",
    "    def backprop_mean_squared_loss(self, y, yhat):\n",
    "        return (yhat - y) * yhat * (1 - yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, arguments) -> None:\n",
    "        self.args_backup = arguments # is saved for later variable purposes\n",
    "        self.n_hidden_layers = arguments.num_layers\n",
    "        self.neurons_h_layers = arguments.hidden_size\n",
    "        self.h_activation_func_name = arguments.activation\n",
    "        self.output_activation_func_name = arguments.output_activation\n",
    "        self.weight_decay = arguments.weight_decay\n",
    "        self.epsilon = arguments.epsilon\n",
    "        self.learning_rate = arguments.learning_rate\n",
    "        self.optimizer_name = arguments.optimizer\n",
    "        self.loss_func_name = arguments.loss\n",
    "        self.batch_size = arguments.batch_size\n",
    "        self.epochs = arguments.epochs\n",
    "        self.dataset_name = arguments.dataset\n",
    "\n",
    "        self.load_dataset()\n",
    "\n",
    "        self.layers = [self.x_train.shape[1]] + [self.neurons_h_layers]*self.n_hidden_layers + [self.y_train[0].shape[0]]\n",
    "        self.n_layers = self.n_hidden_layers + 2\n",
    "\n",
    "        self.activation = ActiviationFunction(arguments.activation)\n",
    "        self.outputActivation = ActiviationFunction(arguments.output_activation)\n",
    "        self.loss = LossFunction(arguments.loss)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        preprocessor = StandardScaler()\n",
    "        if self.dataset_name == \"fashion_mnist\":\n",
    "            from keras.datasets import fashion_mnist\n",
    "            (self.x_train, self.y_train), (self.x_test, self.y_test) = fashion_mnist.load_data()\n",
    "        elif self.dataset_name == \"mnist\":\n",
    "            from keras.datasets import mnist\n",
    "            (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
    "        else:\n",
    "            print(\"404 Dataset Not Found!\")\n",
    "            return\n",
    "\n",
    "        self.x_train = self.x_train.astype('float64')\n",
    "        self.y_train = self.y_train.astype('float64')\n",
    "        self.x_test = self.x_test.astype('float64')\n",
    "        self.y_test = self.y_test.astype('float64')\n",
    "\n",
    "        self.x_train = self.x_train.reshape(self.x_train.shape[0],self.x_train.shape[1]*self.x_train.shape[2])\n",
    "        self.x_train = preprocessor.fit_transform(self.x_train)\n",
    "        self.y_train = self.y_train.reshape(self.y_train.shape[0],1)\n",
    "        self.y_train = to_categorical(self.y_train)\n",
    "\n",
    "        self.x_test = self.x_test.reshape(self.x_test.shape[0],self.x_test.shape[1]*self.x_test.shape[2])\n",
    "        self.x_test = preprocessor.fit_transform(self.x_test)\n",
    "        self.y_test = self.y_test.reshape(self.y_test.shape[0],1)\n",
    "        self.y_test = to_categorical(self.y_test)\n",
    "\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.10, random_state=42)\n",
    "    \n",
    "    def init_parameters(self, debug = False):\n",
    "        if self.args_backup.weight_init == \"random\":\n",
    "            self.randomInit(debug)\n",
    "        elif self.args_backup.weight_init == \"xavier\":\n",
    "            self.xavierInit(debug)\n",
    "    \n",
    "    def randomInit(self, _print=False):\n",
    "        self.parameters = {}\n",
    "        constant = 0.04\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.parameters[f\"W{i}\"] = np.random.randn(self.layers[i], self.layers[i-1])*constant\n",
    "            self.parameters[f\"b{i}\"] = np.zeros((self.layers[i], 1))\n",
    "            if(_print):\n",
    "                print(f'W{i} -> {self.parameters[\"W\" + str(i)].shape}')\n",
    "                print(f'b{i} -> {self.parameters[\"b\" + str(i)].shape}')\n",
    "    \n",
    "    def xavierInit(self, _print=False):\n",
    "        self.parameters = {}\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.parameters[f\"W{i}\"] = np.random.randn(self.layers[i], self.layers[i - 1]) * np.sqrt(2/ (self.layers[i - 1] + self.layers[i]))\n",
    "            self.parameters[f\"b{i}\"] = np.zeros((self.layers[i], 1))\n",
    "            if(_print):\n",
    "                print(f'W{i} -> {self.parameters[\"W\" + str(i)].shape}')\n",
    "                print(f'b{i} -> {self.parameters[\"b\" + str(i)].shape}')\n",
    "\n",
    "    \n",
    "    def gradsInit(self, debug = False):\n",
    "        temp_gradients = {}\n",
    "        for i in range(1, self.n_layers):\n",
    "            temp_gradients[f\"W{i}\"] = np.zeros((self.layers[i], self.layers[i - 1]))\n",
    "            temp_gradients[f\"b{i}\"] = np.zeros((self.layers[i], 1))\n",
    "            if debug:\n",
    "                print(f'W{i} -> {temp_gradients[\"W\" + str(i)].shape}')\n",
    "                print(f'b{i} -> {temp_gradients[\"b\" + str(i)].shape}')\n",
    "        return temp_gradients\n",
    "\n",
    "    \n",
    "    def forward_propagation(self, data):\n",
    "        self.a = {}  # preactivation\n",
    "        self.h = {}  # activation\n",
    "        self.h[\"h0\"] = data.reshape(len(data), 1)\n",
    "        \n",
    "        for i in range(1, self.n_layers - 1):\n",
    "            self.a[f\"a{i}\"] = np.matmul(self.parameters[f\"W{i}\"].T,self.h[f\"h{i-1}\"]) + self.parameters[f\"b{i}\"]\n",
    "            self.h[f\"h{i}\"] = self.activation.activate(self.a[f\"a{i}\"])\n",
    "        \n",
    "        # for output layer\n",
    "        self.a[f\"a{self.n_layers-1}\"] = np.matmul(self.parameters[f\"W{self.n_layers-1}\"].T,self.h[f\"h{self.n_layers-2}\"]) + self.parameters[f\"b{self.n_layers-1}\"]\n",
    "        self.h[f\"h{self.n_layers - 1}\"] = self.outputActivation.activate(self.a[f\"a{i}\"])\n",
    "    \n",
    "\n",
    "    def back_propagation(self, data):\n",
    "        gradients = {}\n",
    "        gradients[f\"a{self.n_layers - 1}\"] = self.loss.getGradient(data.T, self.h[f\"h{self.n_layers - 1}\"])\n",
    "        for i in range(self.n_layers - 1, 1, -1):\n",
    "            gradients[f\"W{i}\"] = np.dot(gradients[f'a{i}'], self.h[f'h{i-1}'])\n",
    "            gradients[[f\"b{i}\"]] = gradients[f\"a{i}\"]\n",
    "            gradients[f\"h{i-1}\"] = np.dot(self.parameters[f\"W{i}\"].T, gradients[f\"a{i}\"])\n",
    "            gradients[f\"a{i-1}\"] = gradients[f\"h{i-1}\"] * self.activation.activate(self.a[f\"a{i-1}\"], backprop=True)\n",
    "        \n",
    "        gradients[f\"W{1}\"] = np.dot(gradients[f'a{1}'], self.h[f'h{1-1}'])\n",
    "        gradients[[f\"b{1}\"]] = gradients[f\"a{1}\"]\n",
    "        gradients[f\"h{1-1}\"] = np.dot(self.parameters[f\"W{1}\"].T, gradients[f\"a{1}\"])\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser(NeuralNetwork):\n",
    "    def __init__(self, arguments):\n",
    "        super().__init__(arguments)\n",
    "        self.parameters_without_activations = None\n",
    "        self.separate_weights_and_biases()\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        for key in self.parameters_without_activations.keys():\n",
    "            self.parameters[key] = self.parameters_without_activations[key]\n",
    "    \n",
    "    def separate_weights_and_biases(self):\n",
    "        for key, value in self.parameters:\n",
    "            if \"W\" in key or \"b\" in key:\n",
    "                self.parameters_without_activations[key] = value\n",
    "    \n",
    "    def generateMetrics(self, x_data, y_data, _type = \"\"):\n",
    "        predictions = []\n",
    "        y = []\n",
    "        yhat = []\n",
    "        loss_history = []\n",
    "        for x,yt in tqdm(zip(x_data , y_data), total=len(y_data), desc=\"Loss calculation\", leave = False):\n",
    "            self.h, self.a = self.forward_propagation(x)\n",
    "            yhat = np.argmax(self.h['h' + str(self.L-1)])\n",
    "            yt = yt.reshape(len(yt),1)\n",
    "            _class = np.argmax(yt)\n",
    "            y.append(_class)\n",
    "            yhat.append(yhat)\n",
    "            predictions.append(yhat == _class)\n",
    "            loss_history.append(self.loss.getLoss(self.h['h' + str(self.L-1)],yt))\n",
    "\n",
    "        accuracy = np.sum(predictions)/len(predictions)\n",
    "        loss = np.sum(loss_history)/len(loss_history)\n",
    "\n",
    "        print(f\"{_type} Accuracy: {accuracy}\", end = \" \")\n",
    "        print(f\"{_type} Loss: {loss}\", end = \"\\n\")\n",
    "\t\t\t\n",
    "        return yhat, y, accuracy*100, loss\n",
    "    \n",
    "    def stochastic_gradient_descent(self):\n",
    "        _parameters = self.parameters_without_activations\n",
    "        for i in tqdm(range(self.epochs), desc=\"Epochs\"):\n",
    "            gradients = self.gradsInit()\n",
    "            for id, x, y in tqdm(enumerate(zip(self.x_train, self.y_train)), desc = \"Optimizer class = Stochastic Gradient Descent\", leave = False):\n",
    "                self.forward_propagation(x)\n",
    "                delta = self.back_propagation(y)\n",
    "                for key in gradients.keys():\n",
    "                    gradients[key] = gradients[key] + delta[key]\n",
    "                \n",
    "                if (id + 1) % self.batch_size == 0:\n",
    "                    for key in _parameters.keys():\n",
    "                        _parameters[key] = _parameters[key] - self.learning_rate * gradients[key]\n",
    "                        gradients = self.gradsInit()\n",
    "        \n",
    "        \n",
    "        self.generateMetrics(self.x_val, self.y_val, _type = \"Validation\")\n",
    "        self.generateMetrics(self.x_val, self.y_val, _type = \"Train\")\n",
    "        self.generateMetrics(self.x_test, self.y_test, _type = \"Test\")\n",
    "\n",
    "        self.update_parameters()\n",
    "    \n",
    "    def moment_based_gradient_descent(self):\n",
    "        self.momentum = self.args_backup.momentum\n",
    "        _parameters = self.parameters_without_activations\n",
    "        _global = self.gradsInit()\n",
    "        for i in tqdm(range(self.epochs), desc=\"Epochs\"):\n",
    "            gradients = self.gradsInit()\n",
    "            lookahead = self.gradsInit()\n",
    "            for id, x, y in tqdm(enumerate(zip(self.x_train, self.y_train)), desc = \"Optimizer class = Momentum Based Gradient Descent\", leave = False):\n",
    "                self.forward_propagation(x)\n",
    "                delta = self.back_propagation(y)\n",
    "                for key in gradients.keys():\n",
    "                    gradients[key] += delta[key]\n",
    "                \n",
    "                if (id + 1) % self.batch_size == 0:\n",
    "                    for key in lookahead.keys():\n",
    "                        lookahead[key] = self.momentum * _global[key] + self.learning_rate * gradients[key]\n",
    "                    \n",
    "                        _parameters[key] = _parameters[key] - lookahead[key]\n",
    "                    \n",
    "                        _global[key] = lookahead[key]\n",
    "                    \n",
    "                    gradients = self.gradsInit()\n",
    "        \n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Validation\")\n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Train\")\n",
    "            self.generateMetrics(self.x_test, self.y_test, _type = \"Test\")\n",
    "            \n",
    "        self.update_parameters()\n",
    "    \n",
    "    def nestrov_gradient_descent(self):\n",
    "        self.momentum = self.args_backup.momentum # beta\n",
    "        _parameters = self.parameters_without_activations\n",
    "        _global = self.gradsInit()\n",
    "        for i in tqdm(range(self.epochs), desc=\"Epochs\"):\n",
    "            gradients = self.gradsInit()\n",
    "            lookahead = self.gradsInit()\n",
    "\n",
    "            # partial init\n",
    "            for key in lookahead.keys():\n",
    "                    lookahead[key] = self.momentum * lookahead[key]\n",
    "            \n",
    "            for id, x, y in tqdm(enumerate(zip(self.x_train, self.y_train)), desc = \"Optimizer class = Nestrov Gradient Descent\", leave = False):\n",
    "                self.forward_propagation(x)\n",
    "                delta = self.back_propagation(y)\n",
    "                for key in gradients.keys():\n",
    "                    gradients[key] += delta[key]\n",
    "\n",
    "                \n",
    "                if (id + 1) % self.batch_size == 0:\n",
    "                    for key in lookahead.keys():\n",
    "                        lookahead[key] = self.momentum * lookahead[key] + self.learning_rate * gradients[key]\n",
    "                    \n",
    "                        _parameters[key] = _parameters[key] - lookahead[key]\n",
    "                    \n",
    "                        _global[key] = lookahead[key]\n",
    "\n",
    "                        gradients = self.gradsInit()\n",
    "\n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Validation\")\n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Train\")\n",
    "            self.generateMetrics(self.x_test, self.y_test, _type = \"Test\")\n",
    "\n",
    "        self.update_parameters()\n",
    "    \n",
    "    def rms_prop(self):\n",
    "        self.beta = self.args_backup.beta\n",
    "        self.epsilon = self.args_backup.epsilon\n",
    "        _parameters = self.parameters_without_activations\n",
    "        _global = self.gradsInit()\n",
    "        for i in tqdm(range(self.epochs), desc=\"Epochs\"):\n",
    "            gradients = self.gradsInit()\n",
    "            for id, x, y in tqdm(enumerate(zip(self.x_train, self.y_train)), desc = \"Optimizer class = RMS Prop\", leave = False):\n",
    "                self.forward_propagation(x)\n",
    "                delta = self.back_propagation(y)\n",
    "                for key in gradients.keys():\n",
    "                    gradients[key] += delta[key]\n",
    "\n",
    "                \n",
    "                if (id + 1) % self.batch_size == 0:\n",
    "                    for key in _global.keys():\n",
    "                        _global[key] = self.beta * _global[key] + (1 - self.beta) * (gradients[key]**2)\n",
    "                    \n",
    "                        _parameters[key] = _parameters[key] - (self.learning_rate * gradients[key])/ (np.sqrt(_global[key] + self.epsilon))\n",
    "                    \n",
    "                    gradients = self.gradsInit()\n",
    "        \n",
    "\n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Validation\")\n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Train\")\n",
    "            self.generateMetrics(self.x_test, self.y_test, _type = \"Test\")\n",
    "\n",
    "        self.update_parameters()\n",
    "    \n",
    "\n",
    "    def adam(self):\n",
    "        self.beta1 = self.args_backup.beta1\n",
    "        self.beta2 = self.args_backup.beta2\n",
    "        self.epsilon = self.args_backup.epsilon\n",
    "\n",
    "        _parameters = self.parameters_without_activations\n",
    "\n",
    "        m_gradients = self.gradsInit()\n",
    "        v_gradients = self.gradsInit()\n",
    "\n",
    "        m_gradients_hat = self.gradsInit()\n",
    "        v_gradients_hat = self.gradsInit()\n",
    "\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Epochs\"):\n",
    "            gradients = self.gradsInit()\n",
    "            for id, x, y in tqdm(enumerate(zip(self.x_train, self.y_train)), desc = \"Optimizer class = Adam\", leave = False):\n",
    "                self.forward_propagation(x)\n",
    "                delta = self.back_propagation(y)\n",
    "                for key in gradients.keys():\n",
    "                    gradients[key] += delta[key]\n",
    "                \n",
    "\n",
    "                if (id+1) % self.batch_size == 0:\n",
    "                    for key in m_gradients.keys():\n",
    "                        m_gradients[key] = self.beta1 * m_gradients[key] + (1 - self.beta1) * gradients[key]\n",
    "                    \n",
    "                        v_gradients[key] = self.beta2 * v_gradients[key] + (1 - self.beta2) * (gradients[key]**2)\n",
    "\n",
    "                    \n",
    "                        m_gradients_hat[key] = m_gradients[key] / (1 - self.beta1 ** (epoch + 1))\n",
    "                        v_gradients_hat[key] = v_gradients[key] / (1 - self.beta2 ** (epoch + 1))\n",
    "\n",
    "                    \n",
    "                    for key in _parameters.keys():\n",
    "                        _parameters[key] = _parameters[key] - (self.learning_rate * m_gradients_hat[key]) / np.sqrt(v_gradients_hat[key] + self.epsilon)\n",
    "                    \n",
    "                    gradients = self.gradsInit()\n",
    "                \n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Validation\")\n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Train\")\n",
    "            self.generateMetrics(self.x_test, self.y_test, _type = \"Test\")\n",
    "\n",
    "        self.update_parameters()\n",
    "    \n",
    "\n",
    "    def nadam(self):\n",
    "        self.beta = self.args_backup.momentum\n",
    "        self.beta1 = self.args_backup.beta1\n",
    "        self.beta2 = self.args_backup.beta2\n",
    "        self.epsilon = self.args_backup.epsilon\n",
    "\n",
    "        _parameters = self.parameters_without_activations\n",
    "        _global = self.gradsInit()\n",
    "\n",
    "        m_gradients = self.gradsInit()\n",
    "        v_gradients = self.gradsInit()\n",
    "\n",
    "        m_gradients_hat = self.gradsInit()\n",
    "        v_gradients_hat = self.gradsInit()\n",
    "\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Epochs\"):\n",
    "            gradients = self.gradsInit()\n",
    "            lookahead = self.gradsInit()\n",
    "\n",
    "            # partial init\n",
    "            for key in lookahead.keys():\n",
    "                lookahead[key] = self.beta * lookahead[key]\n",
    "\n",
    "            for id, x, y in tqdm(enumerate(zip(self.x_train, self.y_train)), desc = \"Optimizer class = nAdam\", leave = False):\n",
    "                self.forward_propagation(x)\n",
    "                delta = self.back_propagation(y)\n",
    "                for key in gradients.keys():\n",
    "                    gradients[key] += delta[key]\n",
    "                \n",
    "\n",
    "                if (id+1) % self.batch_size == 0:\n",
    "                    for key in lookahead.keys():\n",
    "                        lookahead[key] = self.beta * _global[key] + self.learning_rate * gradients[key]\n",
    "                        m_gradients[key] = self.beta1 * m_gradients[key] + (1 - self.beta1) * gradients[key]\n",
    "                    \n",
    "                        v_gradients[key] = self.beta2 * v_gradients[key] + (1 - self.beta2) * (gradients[key]**2)\n",
    "                    \n",
    "                        m_gradients_hat[key] = m_gradients[key] / (1 - self.beta1 ** (epoch + 1))\n",
    "                        v_gradients_hat[key] = v_gradients[key] / (1 - self.beta2 ** (epoch + 1))\n",
    "\n",
    "                        _parameters[key] = _parameters[key] - (self.learning_rate * m_gradients_hat[key]) / np.sqrt(v_gradients_hat + self.epsilon)\n",
    "                    \n",
    "                    for key in _global.keys():\n",
    "                        _global[key] = lookahead[key]\n",
    "\n",
    "                    \n",
    "                    gradients = self.gradsInit()\n",
    "            \n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Validation\")\n",
    "            self.generateMetrics(self.x_val, self.y_val, _type = \"Train\")\n",
    "            self.generateMetrics(self.x_test, self.y_test, _type = \"Test\")\n",
    "\n",
    "        self.update_parameters()\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
