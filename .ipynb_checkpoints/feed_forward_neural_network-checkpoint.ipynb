{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import cli_args\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb.init (\n",
    "# # set the wand project where this run will be logged\n",
    "# project=\"CS6910_Assignment_1\",\n",
    "# )\n",
    "# # Loading the fashion-MNIST dataset\n",
    "# (x_train, y_train), (_test, _test) = fashion_mnist.load_data ()\n",
    "# #class names for fashion-MIST\n",
    "# class_names = ['T-shirt',\n",
    "# 'Trouser',\n",
    "# 'Pullover',\n",
    "# 'Dress',\n",
    "# 'Sandal', 'Shirt',\n",
    "# 'Sneaker',\n",
    "# 'Bag',\n",
    "# 'Coat',\n",
    "# 'Ankle boot']\n",
    "# # creating 2x5 grid\n",
    "# img={}\n",
    "# for i in range(10):\n",
    "#     # to find first image in the training set with class label i\n",
    "#     idx = np.where (y_train == i)[0][0]\n",
    "#     # Plot the image\n",
    "#     img[class_names[i]] = wandb.Image(x_train[idx], caption=class_names[i])\n",
    "# wandb.log(img)\n",
    "# # [optional] finish the wand run, necessary in notebooks\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-wp WANDB_PROJECT] [-we WANDB_ENTITY]\n",
      "                             [-d DATASET] [-e EPOCHS] [-b BATCH_SIZE]\n",
      "                             [-l LOSS] [-o OPTIMIZER] [-lr LEARNING_RATE]\n",
      "                             [-m MOMENTUM] [-beta BETA] [-beta1 BETA1]\n",
      "                             [-beta2 BETA2] [-eps EPSILON] [-w_d WEIGHT_DECAY]\n",
      "                             [-w_i WEIGHT_INIT] [-nhl NUM_LAYERS]\n",
      "                             [-sz HIDDEN_SIZE [HIDDEN_SIZE ...]]\n",
      "                             [-a ACTIVATION] [-oa OUTPUT_ACTIVATION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/vanditshah/Library/Jupyter/runtime/kernel-8ffb2f94-ed55-4ded-845d-51c819fa1f72.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "arguments = cli_args.argumentsIntake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActiviationFunction:\n",
    "    def __init__(self, name):\n",
    "        self.act_name = name\n",
    "    \n",
    "    @property\n",
    "    def setname(self, new_name):\n",
    "        self.act_name = new_name\n",
    "\n",
    "    def activate(self, data, backprop = False):\n",
    "        if self.act_name == \"identity\" and backprop:\n",
    "            pass\n",
    "        elif self.act_name == \"identity\" and not backprop:\n",
    "            return self.identity(data)\n",
    "        elif self.act_name == \"sigmoid\" and backprop:\n",
    "            return self.backprop_sigmoid(data)\n",
    "        elif self.act_name == \"sigmoid\" and not backprop:\n",
    "            return self.sigmoid(data)\n",
    "        elif self.act_name == \"tanh\" and backprop:\n",
    "            return self.backprop_tanh(data)\n",
    "        elif self.act_name == \"tanh\" and not backprop:\n",
    "            return self.tanh(data)\n",
    "        elif self.act_name == \"ReLU\" and backprop:\n",
    "            pass\n",
    "        elif self.act_name == \"ReLU\" and not backprop:\n",
    "            return self.tanh(data)\n",
    "\n",
    "    \n",
    "    def identity(self, data):\n",
    "        return data\n",
    "\n",
    "    def sigmoid(self, data):\n",
    "        return 1/(1 + np.exp(-data))\n",
    "    \n",
    "    def backprop_sigmoid(self, data):\n",
    "        temp = self.sigmoid(data)\n",
    "        return (1 - temp)*temp\n",
    "    \n",
    "    def relu(self, data):\n",
    "        return np.max(0, data)\n",
    "    \n",
    "    def tanh(self, data):\n",
    "        # return (np.exp(data) - np.exp(-data))/(np.exp(data) + np.exp(-data))\n",
    "        return np.tanh(data)\n",
    "    \n",
    "    def backprop_tanh(self, data):\n",
    "        return 1 - np.square(np.tanh(data))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def getLoss(self, y, yhat):\n",
    "        if self.name == \"cross_entropy\":\n",
    "            return self.cross_entropy_loss(y, yhat)\n",
    "        elif self.name == \"mean_squared_error\":\n",
    "            return self.mean_squared_error_loss(y, yhat)\n",
    "    \n",
    "    def getGradient(self, y, yhat):\n",
    "        if self.name == \"cross_entropy\":\n",
    "            return self.backprop_cross_entropy(y, yhat)\n",
    "        elif self.name == \"mean_squared_error\":\n",
    "            return self.backprop_mean_squared_loss(y, yhat)\n",
    "    \n",
    "    def cross_entropy_loss(self, y, yhat):\n",
    "        probability_predicted = yhat[np.argmax(y)]\n",
    "        if probability_predicted <= 0:\n",
    "            probability_predicted += 10**(-9)\n",
    "        return -np.log(probability_predicted)\n",
    "    \n",
    "    def mean_squared_error_loss(self,y, yhat):\n",
    "        return (1/2)*np.sum(np.square(yhat - y))\n",
    "    \n",
    "    def backprop_cross_entropy(self, y, yhat):\n",
    "        return yhat - y\n",
    "    \n",
    "    def backprop_mean_squared_loss(self, y, yhat):\n",
    "        return (yhat - y) * yhat * (1 - yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, arguments) -> None:\n",
    "        self.args_backup = arguments # is saved for later variable purposes\n",
    "        self.n_hidden_layers = arguments.num_layers\n",
    "        self.neurons_h_layers = arguments.hidden_size\n",
    "        self.h_activation_func_name = arguments.activation\n",
    "        self.output_activation_func_name = arguments.output_activation\n",
    "        self.weight_decay = arguments.weight_decay\n",
    "        self.epsilon = arguments.epsilon\n",
    "        self.learning_rate = arguments.learning_rate\n",
    "        self.optimizer_name = arguments.optimizer\n",
    "        self.loss_func_name = arguments.loss\n",
    "        self.batch_size = arguments.batch_size\n",
    "        self.epochs = arguments.epochs\n",
    "        self.dataset_name = arguments.dataset\n",
    "\n",
    "        self.load_dataset()\n",
    "\n",
    "        self.layers = [self.x_train.shape[1]] + self.neurons_h_layers + [self.y_train[0].shape[0]]\n",
    "        self.n_layers = self.n_hidden_layers + 2\n",
    "\n",
    "        self.activation = ActiviationFunction(arguments.activation)\n",
    "        self.outputActivation = ActiviationFunction(arguments.output_activation)\n",
    "        self.loss = LossFunction(arguments.loss)\n",
    "\n",
    "    def load_dataset(self):\n",
    "        preprocessor = StandardScaler()\n",
    "        if self.dataset_name == \"fashion_mnist\":\n",
    "            from keras.datasets import fashion_mnist\n",
    "            (self.x_train, self.y_train), (self.x_test, self.y_test) = fashion_mnist.load_data()\n",
    "\n",
    "            self.x_train = self.x_train.astype('float64')\n",
    "            self.y_train = self.y_train.astype('float64')\n",
    "            self.x_test = self.x_test.astype('float64')\n",
    "            self.y_test = self.y_test.astype('float64')\n",
    "\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0],self.x_train.shape[1]*self.x_train.shape[2])\n",
    "            self.x_train = preprocessor.fit_transform(self.x_train)\n",
    "            self.y_train = self.y_train.reshape(self.y_train.shape[0],1)\n",
    "            self.y_train = to_categorical(self.y_train)\n",
    "\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0],self.x_test.shape[1]*self.x_test.shape[2])\n",
    "            self.x_test = preprocessor.fit_transform(self.x_test)\n",
    "            self.y_test = self.y_test.reshape(self.y_test.shape[0],1)\n",
    "            self.y_test = to_categorical(self.y_test)\n",
    "\n",
    "            self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.10, random_state=42)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def init_parameters(self, debug = False):\n",
    "        if self.args_backup.weight_init == \"random\":\n",
    "            self.randomInit(debug)\n",
    "        elif self.args_backup.weight_init == \"xavier\":\n",
    "            self.xavierInit(debug)\n",
    "    \n",
    "    def randomInit(self, _print=False):\n",
    "        self.parameters = {}\n",
    "        constant = 0.04\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.parameters[f\"W{i}\"] = np.random.randn(self.layers[i], self.layers[i-1])*constant\n",
    "            self.parameters[f\"b{i}\"] = np.zeros((self.layers[i], 1))\n",
    "            if(_print):\n",
    "                print(f'W{i} -> {self.parameters[\"W\" + str(i)].shape}')\n",
    "                print(f'b{i} -> {self.parameters[\"b\" + str(i)].shape}')\n",
    "    \n",
    "    def xavierInit(self, _print=False):\n",
    "        self.parameters = {}\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.parameters[f\"W{i}\"] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i - 1]) * np.sqrt(2/ (self.layer_sizes[i - 1] + self.layer_sizes[i]))\n",
    "            self.parameters[f\"b{i}\"] = np.zeros((self.layers[i], 1))\n",
    "            if(_print):\n",
    "                print(f'W{i} -> {self.parameters[\"W\" + str(i)].shape}')\n",
    "                print(f'b{i} -> {self.parameters[\"b\" + str(i)].shape}')\n",
    "\n",
    "    \n",
    "    def gradsInit(self, debug = False):\n",
    "        temp_gradients = {}\n",
    "        for i in range(1, self.n_layers):\n",
    "            temp_gradients[f\"W{i}\"] = np.zeros((self.layers[i], self.layers[i - 1]))\n",
    "            temp_gradients[f\"b{i}\"] = np.zeros((self.layers[i], 1))\n",
    "            if debug:\n",
    "                print(f'W{i} -> {temp_gradients[\"W\" + str(i)].shape}')\n",
    "                print(f'b{i} -> {temp_gradients[\"b\" + str(i)].shape}')\n",
    "        return temp_gradients\n",
    "\n",
    "    \n",
    "    def forward_propagation(self, data):\n",
    "        self.a = {}  # preactivation\n",
    "        self.h = {}  # activation\n",
    "        self.h[\"h0\"] = data.T\n",
    "        \n",
    "        for i in range(1, self.n_layers - 1):\n",
    "            self.a[f\"a{i}\"] = np.matmul(self.parameters[f\"W{i}\"].T,self.h[f\"h{i-1}\"]) + self.parameters[f\"b{i}\"]\n",
    "            self.h[f\"h{i}\"] = self.activation.activate(self.a[f\"a{i}\"])\n",
    "        \n",
    "        # for output layer\n",
    "        self.a[f\"a{self.n_layers-1}\"] = np.matmul(self.parameters[f\"W{self.n_layers-1}\"].T,self.h[f\"h{self.n_layers-2}\"]) + self.parameters[f\"b{self.n_layers-1}\"]\n",
    "        self.h[f\"h{self.n_layers - 1}\"] = self.outputActivation.activate(self.a[f\"a{i}\"])\n",
    "    \n",
    "\n",
    "    def back_propagation(self, data):\n",
    "        gradients = {}\n",
    "        gradients[f\"a{self.n_layers - 1}\"] = self.loss.getGradient(data.T, self.h[f\"h{self.n_layers - 1}\"])\n",
    "        for i in range(self.n_layers - 1, 1, -1):\n",
    "            gradients[f\"W{i}\"] = np.outer(gradients[f'a{i}'], self.h[f'h{i-1}'])\n",
    "            gradients[[f\"b{i}\"]] = gradients[f\"a{i}\"]\n",
    "            gradients[f\"h{i-1}\"] = np.dot(self.parameters[f\"W{i}\"].T, gradients[f\"a{i}\"])\n",
    "            gradients[f\"a{i-1}\"] = gradients[f\"h{i-1}\"] * self.activation.activate(self.a[f\"a{i-1}\"], backprop=True)\n",
    "        \n",
    "        gradients[f\"W{1}\"] = np.outer(gradients[f'a{1}'], self.h[f'h{1-1}'])\n",
    "        gradients[[f\"b{1}\"]] = gradients[f\"a{1}\"]\n",
    "        gradients[f\"h{1-1}\"] = np.dot(self.parameters[f\"W{1}\"].T, gradients[f\"a{1}\"])\n",
    "\n",
    "        return gradients\n",
    "    \n",
    "    def generateMetrics(self):\n",
    "        pass\n",
    "\n",
    "    def stochastic_gradient_descent(self):\n",
    "        for i in tqdm(range(self.epochs), desc=\"Epochs\"):\n",
    "            for id, x, y in tqdm(enumerate(zip(self.x_train, self.y_train)), desc = \"Optimizer Algorithm\"):\n",
    "                self.forward_propagation(x)\n",
    "                gradients = self.back_propagation(y)\n",
    "                for key in self.parameters.keys():\n",
    "                    self.parameters[key] = self.parameters[key] - self.learning_rate * gradients[key]\n",
    "            \n",
    "            # TO DO: Print Accuracy for each epoch\n",
    "    \n",
    "    def moment_based_gradient_descent(self):\n",
    "        self.momentum = self.args_backup.momentum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5fc4a3749a4a4e958565455e7e97af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs \n",
       ":   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6416b7e888e84617b24f448f103ec2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e891b53174f4ea7a10f701ea47bee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m), desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mEpochs \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m), desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGradient Descent\u001b[39m\u001b[39m\"\u001b[39m, leave \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> 6\u001b[0m         sleep(\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "\n",
    "for i in tqdm(range(4), desc = \"Epochs \\n\"):\n",
    "    for j in tqdm(range(500), desc = \"Gradient Descent\", leave = False):\n",
    "        sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
